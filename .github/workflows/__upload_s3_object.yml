name: __upload_s3_object.yml

on:
  workflow_call:
    inputs:
      aws_account_id:
        description: ID of the AWS Account where resources will be deployed.
        required: true
        type: string

      aws_region:
        description: AWS region where IAM role lives.
        required: true
        type: string

      bucket_name:
        description: Name of S3 bucket where the object will be deployed.
        required: true
        type: string

      environment:
        description: Deployment environment (used for environment protection and concurrency).  
        required: false
        type: string

      object_key:
        description: Path where object will be located in the S3 bucket.
        required: true
        type: string

      object_name:
        description: Name of the S3 object.
        required: true
        type: string

      role_name:
        description: Name of IAM role to assume.
        required: true
        type: string

      role_session_duration:
        description: Length of time in seconds assumed role token will be valid.
        required: false
        type: number
        default: 900

      role_session_name:
        description: Name of IAM role session. Defaults to '[repo name]+run=[run id]-[run number]+[triggering actor]'.
        required: false
        type: string
        default: ${{ github.event.repository.name }}+run=${{ github.run_id }}-${{ github.run_number }}+${{ github.triggering_actor }}


jobs:
  upload:
    name: Upload S3 Object
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    concurrency: upload-${{ inputs.environment }}-${{ inputs.object_name }}
    steps:
      - name: Download Object Artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.object_name }}
          path: artifacts

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ inputs.aws_account_id }}:role/${{ inputs.role_name }}
          aws-region: ${{ inputs.aws_region }}
          role-duration-seconds: ${{ inputs.role_session_duration }}
          role-session-name: ${{ inputs.role_session_name }}

      - name: Check Object for Changes
        id: md5-check
        working-directory: artifacts
        run: |

          # Calculate the local file's hash
          LOCAL_HASH=$(md5sum "${{ inputs.object_name }}" | awk '{ print $1 }')

          # Allow non-zero exit code without failure
          set +e

          # Try to retrieve the existing file's custom metadata (content hash) from S3
          EXISTING_HASH=$(aws s3api head-object \
            --bucket "${{ inputs.bucket_name }}" \
            --key "${{ inputs.object_key }}" \
            --query "Metadata.content-md5" --output text 2>/dev/null)

          # Capture exit code
          RESULT=$?

          # Reset default exit code failure behavior
          set -e

          # Check if the command to retrieve metadata was successful (file exists)
          # and if hashes match
          if [[ $RESULT -eq 0 && "$LOCAL_HASH" == "$EXISTING_HASH" ]]; then
            echo "Local file md5 matches the existing S3 object's metadata."
            echo "Skipping upload."
            matches=true
          else
            matches=false
          fi

          echo "MATCHES = ${matches}"
          echo "MATCHES=${matches}" >> $GITHUB_OUTPUT

      - name: Upload to S3
        if: steps.md5-check.outputs.MATCHES == 'false'
        working-directory: artifacts
        run: |

          echo "Local file differs from the existing S3 object or the object does not exist."
          echo "Uploading..."

          # Upload with custom metadata
          aws s3 cp "${{ inputs.object_name }}" \
            "s3://${{ inputs.bucket_name }}/${{ inputs.object_key }}" \
            --metadata content-md5="$LOCAL_HASH"
